{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3347069,"sourceType":"datasetVersion","datasetId":2020131}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision opencv-python numpy pandas matplotlib scikit-learn torchsummary kagglehub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T02:01:08.333261Z","iopub.execute_input":"2025-05-07T02:01:08.333499Z","iopub.status.idle":"2025-05-07T02:02:25.213286Z","shell.execute_reply.started":"2025-05-07T02:01:08.333477Z","shell.execute_reply":"2025-05-07T02:02:25.212375Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: torchsummary in /usr/local/lib/python3.11/dist-packages (1.5.1)\nRequirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.11)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport h5py\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\nimport kagglehub\nimport glob\nimport random\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc, classification_report\nfrom sklearn.preprocessing import label_binarize\nfrom itertools import cycle\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Set random seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Configuration parameters\nCONFIG = {\n    'batch_size': 16,\n    'num_epochs': 15,  # Increased from 10 to 15\n    'base_lr': 5e-4,   # Changed from 1e-4\n    'weight_decay': 2e-5,  # Changed from 1e-4\n    'scheduler': 'onecycle',  # Options: 'cosine', 'onecycle'\n    'mixup_alpha': 0.2,  # Added mixup augmentation\n    'label_smoothing': 0.1,  # Added label smoothing\n    'dropout_rate': 0.4,  # Increased dropout\n    'model_variant': 'efficient_cbam',  # Options: 'efficient_basic', 'efficient_cbam', 'efficient_dual'\n    'num_classes': 3\n}\n\nprint(\"Training configuration:\")\nfor key, value in CONFIG.items():\n    print(f\"  {key}: {value}\")\n\n# Download the dataset using kagglehub\nprint(\"Downloading dataset...\")\ndataset_path = kagglehub.dataset_download(\"ashkhagan/figshare-brain-tumor-dataset\")\nprint(\"Path to dataset files:\", dataset_path)\n\n# Data loading functions\ndef load_data():\n    \"\"\"Load and preprocess data from the downloaded directory\"\"\"\n    # First find all .mat files in the dataset directory and its subdirectories\n    mat_files = []\n    for root, dirs, files in os.walk(dataset_path):\n        for file in files:\n            if file.endswith('.mat') and file != 'cvind.mat':\n                mat_files.append(os.path.join(root, file))\n   \n    print(f\"Found {len(mat_files)} .mat files\")\n   \n    # If we don't have enough files, exit\n    if len(mat_files) < 3000:\n        raise ValueError(f\"Expected ~3064 .mat files but found only {len(mat_files)}\")\n   \n    # Sort the files to ensure consistent order\n    mat_files.sort()\n   \n    # Prepare arrays for images and labels\n    img = np.zeros((len(mat_files), 224, 224))\n    lbl = []\n   \n    # Load each file\n    for i, file_path in enumerate(mat_files):\n        try:\n            with h5py.File(file_path, 'r') as f:\n                images = f['cjdata']\n                resized = cv2.resize(images['image'][:,:], (224, 224), interpolation=cv2.INTER_CUBIC)\n                x = np.asarray(resized)\n                x = (x - np.min(x)) / (np.max(x) - np.min(x))  # Normalization\n                x = x.reshape((1, 224, 224))\n                img[i] = x\n                lbl.append(int(images['label'][0]))\n               \n                if i % 500 == 0:\n                    print(f\"Processed {i} images\")\n        except Exception as e:\n            print(f\"Failed to load image at {file_path}: {e}\")\n   \n    # Find cvind.mat file\n    cvind_files = []\n    for root, dirs, files in os.walk(dataset_path):\n        for file in files:\n            if file == 'cvind.mat':\n                cvind_files.append(os.path.join(root, file))\n   \n    if not cvind_files:\n        raise ValueError(\"Could not find cvind.mat file\")\n   \n    cvind_path = cvind_files[0]\n    print(f\"Found cvind.mat at: {cvind_path}\")\n   \n    # Load fold indices\n    with h5py.File(cvind_path, 'r') as f:\n        idx = np.array(f['cvind']).astype(np.int16).squeeze()\n   \n    return img, np.array(lbl), idx\n\n# Custom Dataset\nclass BrainTumorDataset(Dataset):\n    def __init__(self, images, labels, transform=None):\n        self.images = images\n        self.labels = labels\n        self.transform = transform\n       \n    def __len__(self):\n        return len(self.labels)\n   \n    def __getitem__(self, idx):\n        # Convert grayscale to RGB by repeating channel\n        image = self.images[idx]\n        image = np.repeat(image.reshape(224, 224, 1), 3, axis=2)\n        label = self.labels[idx] - 1  # Convert to 0-indexed\n       \n        if self.transform:\n            image = self.transform(image)\n        else:\n            image = torch.from_numpy(image.transpose(2, 0, 1)).float()\n           \n        return image, label\n\n# Define transforms for training and validation with more aggressive augmentation\ndef get_transforms():\n    train_transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomVerticalFlip(p=0.5),\n        transforms.RandomRotation(30),  # Increased from 15 to 30\n        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),  # Added scale\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Added color jitter\n        transforms.RandomPerspective(distortion_scale=0.2, p=0.5),  # Added perspective transform\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        transforms.RandomErasing(p=0.2)  # Added random erasing\n    ])\n   \n    val_transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n   \n    return train_transform, val_transform\n\n# Get train and test splits\ndef get_train_test_data(images, labels, fold_indices, test_fold):\n    train_mask = fold_indices != test_fold\n    test_mask = fold_indices == test_fold\n   \n    train_images = images[train_mask]\n    train_labels = labels[train_mask]\n    test_images = images[test_mask]\n    test_labels = labels[test_mask]\n   \n    return (train_images, train_labels), (test_images, test_labels)\n\n# MixUp augmentation\ndef mixup_data(x, y, alpha=1.0):\n    '''Compute the mixup data. Return mixed inputs, pairs of targets, and lambda'''\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    batch_size = x.size()[0]\n    index = torch.randperm(batch_size).to(x.device)\n\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\ndef mixup_criterion(criterion, pred, y_a, y_b, lam):\n    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n\n# Squeeze and Excitation Block\nclass SEBlock(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SEBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y.expand_as(x)\n\n# CBAM: Convolutional Block Attention Module\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_channels, reduction_ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n       \n        self.fc = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels // reduction_ratio, 1, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels // reduction_ratio, in_channels, 1, bias=False)\n        )\n       \n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        out = avg_out + max_out\n        return torch.sigmoid(out)\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n        padding = 3 if kernel_size == 7 else 1\n       \n        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n       \n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        out = torch.cat([avg_out, max_out], dim=1)\n        out = self.conv(out)\n        return torch.sigmoid(out)\n\nclass CBAM(nn.Module):\n    def __init__(self, in_channels, reduction_ratio=16, kernel_size=7):\n        super(CBAM, self).__init__()\n        self.channel_attention = ChannelAttention(in_channels, reduction_ratio)\n        self.spatial_attention = SpatialAttention(kernel_size)\n       \n    def forward(self, x):\n        x = x * self.channel_attention(x)\n        x = x * self.spatial_attention(x)\n        return x\n\n# Self-Attention Block\nclass SelfAttention(nn.Module):\n    def __init__(self, in_channels):\n        super(SelfAttention, self).__init__()\n        self.query = nn.Conv2d(in_channels, in_channels//8, kernel_size=1)\n        self.key = nn.Conv2d(in_channels, in_channels//8, kernel_size=1)\n        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n        self.softmax = nn.Softmax(dim=-1)\n       \n    def forward(self, x):\n        batch_size, C, width, height = x.size()\n       \n        # Reshape for matrix multiplication\n        proj_query = self.query(x).view(batch_size, -1, width*height).permute(0, 2, 1)  # B X (W*H) X C\n        proj_key = self.key(x).view(batch_size, -1, width*height)  # B X C X (W*H)\n       \n        # Calculate attention map\n        attention = torch.bmm(proj_query, proj_key)  # B X (W*H) X (W*H)\n        attention = self.softmax(attention)\n       \n        # Apply attention to values\n        proj_value = self.value(x).view(batch_size, -1, width*height)  # B X C X (W*H)\n        out = torch.bmm(proj_value, attention.permute(0, 2, 1))  # B X C X (W*H)\n        out = out.view(batch_size, C, width, height)  # B X C X W X H\n       \n        # Add residual connection with learnable parameter gamma\n        out = self.gamma * out + x\n       \n        return out\n\n# Dual Path Block - combines features from multiple paths\nclass DualPathBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(DualPathBlock, self).__init__()\n       \n        # First path - standard convolution\n        self.conv_path = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n       \n        # Second path - depthwise separable convolution\n        self.dw_path = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False),\n            nn.BatchNorm2d(in_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n       \n        # Attention module\n        self.cbam = CBAM(out_channels)\n       \n        # Residual connection if dimensions change\n        self.downsample = None\n        if stride != 1 or in_channels != out_channels:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n   \n    def forward(self, x):\n        residual = x\n       \n        # Process through both paths\n        out1 = self.conv_path(x)\n        out2 = self.dw_path(x)\n       \n        # Combine paths with element-wise addition\n        out = out1 + out2\n       \n        # Apply attention\n        out = self.cbam(out)\n       \n        # Apply residual connection\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        out += residual\n       \n        return out\n\n# Feature Pyramid Network (FPN) module\nclass FPN(nn.Module):\n    def __init__(self, channels):\n        super(FPN, self).__init__()\n        self.lateral_conv = nn.Conv2d(channels, 256, kernel_size=1)\n        self.output_conv = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n       \n    def forward(self, x):\n        lateral = self.lateral_conv(x)\n        output = self.output_conv(lateral)\n        return output\n\n# Main model with EfficientNet backbone and attention - Modified to support different variants\nclass EnhancedEfficientNetClassifier(nn.Module):\n    def __init__(self, model_variant='efficient_basic', num_classes=3, dropout_rate=0.3):\n        super(EnhancedEfficientNetClassifier, self).__init__()\n       \n        # Load pretrained EfficientNet and remove final classifier\n        # Using B4 instead of B3 for higher capacity\n        efficient_net = models.efficientnet_b4(weights=\"IMAGENET1K_V1\")\n        self.features = nn.Sequential(*list(efficient_net.children())[:-1])\n       \n        # Get the output feature dimension\n        feature_dim = efficient_net._modules['classifier'][1].in_features\n        split_dim = feature_dim // 2\n       \n        self.model_variant = model_variant\n       \n        if model_variant == 'efficient_basic':\n            # Basic variant with Self-Attention and SE blocks\n            self.attention1 = SelfAttention(split_dim)\n            self.attention2 = SelfAttention(split_dim)\n            self.se1 = SEBlock(split_dim)\n            self.se2 = SEBlock(split_dim)\n           \n        elif model_variant == 'efficient_cbam':\n            # Enhanced variant with CBAM\n            self.cbam1 = CBAM(split_dim)\n            self.cbam2 = CBAM(split_dim)\n           \n        elif model_variant == 'efficient_dual':\n            # Dual path variant\n            self.dual_path1 = DualPathBlock(split_dim, split_dim)\n            self.dual_path2 = DualPathBlock(split_dim, split_dim)\n           \n        # FPN modules for multi-scale feature extraction\n        self.fpn1 = FPN(split_dim)\n        self.fpn2 = FPN(split_dim)\n       \n        # Global pooling\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n       \n        # Additional features: Global Context Block\n        self.gcb = nn.Sequential(\n            nn.Conv2d(512, 64, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(64, 512, kernel_size=1),\n            nn.Sigmoid()\n        )\n       \n        # Fully connected layers with improved regularization\n        self.classifier = nn.Sequential(\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),  # Increased dropout\n            nn.Linear(512, 256),  # Added an extra FC layer\n            nn.BatchNorm1d(256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate/2),  # Lower dropout in the last layer\n            nn.Linear(256, num_classes)\n        )\n   \n    def forward(self, x):\n        # Extract features using EfficientNet backbone\n        features = self.features(x)\n       \n        # Split features into two parts\n        features1, features2 = torch.split(features, features.size(1)//2, dim=1)\n       \n        # Apply attention based on model variant\n        if self.model_variant == 'efficient_basic':\n            features1 = self.attention1(features1)\n            features1 = self.se1(features1)\n           \n            features2 = self.attention2(features2)\n            features2 = self.se2(features2)\n           \n        elif self.model_variant == 'efficient_cbam':\n            features1 = self.cbam1(features1)\n            features2 = self.cbam2(features2)\n           \n        elif self.model_variant == 'efficient_dual':\n            features1 = self.dual_path1(features1)\n            features2 = self.dual_path2(features2)\n       \n        # Apply FPN for multi-scale feature enhancement\n        features1 = self.fpn1(features1)\n        features2 = self.fpn2(features2)\n       \n        # Concatenate features\n        combined_features = torch.cat([features1, features2], dim=1)\n       \n        # Apply global context\n        context = self.gcb(combined_features)\n        combined_features = combined_features * context\n       \n        # Global pooling\n        pooled = self.global_pool(combined_features)\n        pooled = pooled.view(pooled.size(0), -1)\n       \n        # Classification\n        output = self.classifier(pooled)\n       \n        return output\n\n# Training function with mixup and label smoothing\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10, mixup_alpha=0.0):\n    best_val_acc = 0.0\n    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n   \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print('-' * 10)\n       \n        # Training phase\n        model.train()\n        running_loss = 0.0\n        running_corrects = 0\n        total_samples = 0\n       \n        for inputs, labels in train_loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n           \n            # Apply mixup if alpha > 0\n            if mixup_alpha > 0:\n                inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, mixup_alpha)\n                use_mixup = True\n            else:\n                use_mixup = False\n           \n            # Zero the parameter gradients\n            optimizer.zero_grad()\n           \n            # Forward pass\n            outputs = model(inputs)\n           \n            # Calculate loss with or without mixup\n            if use_mixup:\n                loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n                # For accuracy calculation with mixup, we use the dominant label\n                _, preds = torch.max(outputs, 1)\n                running_corrects += (lam * torch.sum(preds == labels_a) +\n                                   (1 - lam) * torch.sum(preds == labels_b)).float()\n            else:\n                loss = criterion(outputs, labels)\n                _, preds = torch.max(outputs, 1)\n                running_corrects += torch.sum(preds == labels.data)\n           \n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n           \n            # Statistics\n            batch_size = inputs.size(0)\n            running_loss += loss.item() * batch_size\n            total_samples += batch_size\n       \n        # Step scheduler if it's not OneCycleLR (which steps per batch)\n        if CONFIG['scheduler'] != 'onecycle':\n            scheduler.step()\n       \n        epoch_loss = running_loss / total_samples\n        epoch_acc = running_corrects.double() / total_samples\n       \n        history['train_loss'].append(epoch_loss)\n        history['train_acc'].append(epoch_acc.cpu().numpy())\n       \n        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n       \n        # Validation phase\n        model.eval()\n        val_running_loss = 0.0\n        val_running_corrects = 0\n        val_total_samples = 0\n       \n        # No gradient calculation needed for validation\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n               \n                # Forward pass\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                loss = criterion(outputs, labels)\n               \n                # Statistics\n                batch_size = inputs.size(0)\n                val_running_loss += loss.item() * batch_size\n                val_running_corrects += torch.sum(preds == labels.data)\n                val_total_samples += batch_size\n       \n        val_epoch_loss = val_running_loss / val_total_samples\n        val_epoch_acc = val_running_corrects.double() / val_total_samples\n       \n        history['val_loss'].append(val_epoch_loss)\n        history['val_acc'].append(val_epoch_acc.cpu().numpy())\n       \n        print(f'Val Loss: {val_epoch_loss:.4f} Acc: {val_epoch_acc:.4f}')\n       \n        # Save best model\n        if val_epoch_acc > best_val_acc:\n            best_val_acc = val_epoch_acc\n            torch.save(model.state_dict(), f'best_{CONFIG[\"model_variant\"]}_model.pth')\n            print(f'New best model saved with accuracy: {val_epoch_acc:.4f}')\n       \n        print()\n   \n    # Load best model weights\n    model.load_state_dict(torch.load(f'best_{CONFIG[\"model_variant\"]}_model.pth'))\n    return model, history\n\n# Evaluation function with advanced metrics\ndef evaluate_model(model, test_loader, num_classes=3):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    all_probs = []\n   \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            probs = F.softmax(outputs, dim=1)\n            _, preds = torch.max(outputs, 1)\n           \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.numpy())\n            all_probs.extend(probs.cpu().numpy())\n   \n    # Convert to numpy arrays\n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n    all_probs = np.array(all_probs)\n   \n    # Calculate metrics\n    accuracy = accuracy_score(all_labels, all_preds)\n   \n    # Confusion Matrix\n    cm = confusion_matrix(all_labels, all_preds)\n   \n    # Classification Report\n    report = classification_report(all_labels, all_preds)\n   \n    # ROC Curve and AUC\n    # Binarize the labels for ROC calculation\n    labels_bin = label_binarize(all_labels, classes=range(num_classes))\n   \n    # Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i in range(num_classes):\n        fpr[i], tpr[i], _ = roc_curve(labels_bin[:, i], all_probs[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n   \n    # Compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(labels_bin.ravel(), all_probs.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n   \n    # Compute macro-average ROC curve and ROC area\n    # First aggregate all false positive rates\n    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n   \n    # Then interpolate all ROC curves at these points\n    mean_tpr = np.zeros_like(all_fpr)\n    for i in range(num_classes):\n        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n   \n    # Finally average it and compute AUC\n    mean_tpr /= num_classes\n   \n    fpr[\"macro\"] = all_fpr\n    tpr[\"macro\"] = mean_tpr\n    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n   \n    # Plot ROC Curves\n    plt.figure(figsize=(12, 8))\n   \n    # Plot micro-average ROC curve\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n             label=f'Micro-average ROC curve (area = {roc_auc[\"micro\"]:.2f})',\n             color='deeppink', linestyle=':', linewidth=4)\n   \n    # Plot macro-average ROC curve\n    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n             label=f'Macro-average ROC curve (area = {roc_auc[\"macro\"]:.2f})',\n             color='navy', linestyle=':', linewidth=4)\n   \n    # Plot ROC curves for all classes\n    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n    class_names = ['Meningioma', 'Glioma', 'Pituitary']\n   \n    for i, color, name in zip(range(num_classes), colors, class_names):\n        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n                 label=f'ROC curve of {name} (area = {roc_auc[i]:.2f})')\n   \n    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(f'ROC Curve - {CONFIG[\"model_variant\"]}')\n    plt.legend(loc=\"lower right\")\n    plt.savefig(f'{CONFIG[\"model_variant\"]}_roc_curve.png')\n    plt.close()\n   \n    # Plot Confusion Matrix\n    plt.figure(figsize=(10, 8))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(f'Confusion Matrix - {CONFIG[\"model_variant\"]}')\n    plt.colorbar()\n   \n    tick_marks = np.arange(num_classes)\n    plt.xticks(tick_marks, class_names, rotation=45)\n    plt.yticks(tick_marks, class_names)\n   \n    # Add text annotations to the confusion matrix\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(j, i, format(cm[i, j], 'd'),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n   \n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    plt.savefig(f'{CONFIG[\"model_variant\"]}_confusion_matrix.png')\n    plt.close()\n   \n    # Return metrics and predictions\n    return {\n        'accuracy': accuracy,\n        'confusion_matrix': cm,\n        'classification_report': report,\n        'roc_auc': roc_auc,\n        'predictions': all_preds,\n        'labels': all_labels,\n        'probabilities': all_probs\n    }\n\n# Function to test with a random image\ndef test_random_image(model, images, labels, transform=None):\n    # Choose a random image\n    idx = random.randint(0, len(images) - 1)\n    img = images[idx]\n    true_label = labels[idx] - 1  # Convert totrue_label = labels[idx] - 1  # Convert to 0-indexed\n   \n    # Preprocess the image\n    img = np.repeat(img.reshape(224, 224, 1), 3, axis=2)\n   \n    # Apply transform if provided\n    if transform:\n        img_tensor = transform(img).unsqueeze(0).to(device)  # Add batch dimension\n    else:\n        img_tensor = torch.from_numpy(img.transpose(2, 0, 1)).float().unsqueeze(0).to(device)\n   \n    # Get model prediction\n    model.eval()\n    with torch.no_grad():\n        output = model(img_tensor)\n        probs = F.softmax(output, dim=1)\n        _, pred = torch.max(output, 1)\n   \n    # Display the image and prediction\n    class_names = ['Meningioma', 'Glioma', 'Pituitary']\n    plt.figure(figsize=(6, 6))\n    plt.imshow(img, cmap='gray')\n    plt.title(f'True: {class_names[true_label]}, Pred: {class_names[pred.item()]}\\nConfidence: {probs[0][pred.item()]:.4f}')\n    plt.axis('off')\n    plt.savefig(f'{CONFIG[\"model_variant\"]}_random_test.png')\n    plt.close()\n   \n    print(f\"Random image test - True: {class_names[true_label]}, Predicted: {class_names[pred.item()]}\")\n    print(f\"Confidence scores: {probs[0].cpu().numpy()}\")\n   \n    return {\n        'image_idx': idx,\n        'true_label': true_label,\n        'predicted_label': pred.item(),\n        'confidence': probs[0][pred.item()].item(),\n        'all_probs': probs[0].cpu().numpy()\n    }\n\n# Main execution function\ndef run_experiment(fold_index):\n    print(f\"\\n{'='*20} RUNNING FOLD {fold_index} {'='*20}\\n\")\n   \n    # Load data\n    images, labels, fold_indices = load_data()\n   \n    # Get train and test data for this fold\n    (train_images, train_labels), (test_images, test_labels) = get_train_test_data(\n        images, labels, fold_indices, fold_index)\n   \n    # Create datasets with transforms\n    train_transform, val_transform = get_transforms()\n   \n    train_dataset = BrainTumorDataset(train_images, train_labels, transform=train_transform)\n    test_dataset = BrainTumorDataset(test_images, test_labels, transform=val_transform)\n   \n    # Create data loaders with larger batch size\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=CONFIG['batch_size'],\n        shuffle=True,\n        num_workers=4,\n        pin_memory=True  # Add pin_memory for faster data transfer to GPU\n    )\n   \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=CONFIG['batch_size'],\n        shuffle=False,\n        num_workers=4,\n        pin_memory=True\n    )\n   \n    # Initialize model based on selected variant\n    model = EnhancedEfficientNetClassifier(\n        model_variant=CONFIG['model_variant'],\n        num_classes=CONFIG['num_classes'],\n        dropout_rate=CONFIG['dropout_rate']\n    )\n    model = model.to(device)\n   \n    # Loss function with label smoothing\n    criterion = nn.CrossEntropyLoss(label_smoothing=CONFIG['label_smoothing'])\n   \n    # Optimizer with weight decay\n    optimizer = optim.AdamW(\n        model.parameters(),\n        lr=CONFIG['base_lr'],\n        weight_decay=CONFIG['weight_decay']\n    )\n   \n    # Set scheduler based on configuration\n    if CONFIG['scheduler'] == 'cosine':\n        scheduler = CosineAnnealingLR(\n            optimizer,\n            T_max=CONFIG['num_epochs'],\n            eta_min=CONFIG['base_lr'] / 100\n        )\n    elif CONFIG['scheduler'] == 'onecycle':\n        # OneCycle learning rate scheduler\n        scheduler = OneCycleLR(\n            optimizer,\n            max_lr=CONFIG['base_lr'] * 10,  # Peak LR is 10x the base LR\n            steps_per_epoch=len(train_loader),\n            epochs=CONFIG['num_epochs'],\n            pct_start=0.3,  # Spend 30% of time increasing LR\n            div_factor=25.0,  # Initial LR = max_lr/25\n            final_div_factor=10000.0  # Final LR = initial_lr/10000\n        )\n   \n    # Train model with specified epochs\n    start_time = time.time()\n    model, history = train_model(\n        model,\n        train_loader,\n        test_loader,\n        criterion,\n        optimizer,\n        scheduler,\n        num_epochs=CONFIG['num_epochs'],\n        mixup_alpha=CONFIG['mixup_alpha']\n    )\n    end_time = time.time()\n   \n    # Evaluate model with advanced metrics\n    metrics = evaluate_model(model, test_loader, num_classes=CONFIG['num_classes'])\n    accuracy = metrics['accuracy']\n   \n    print(f\"Fold {fold_index} accuracy: {accuracy:.4f}\")\n    print(f\"Training time: {end_time - start_time:.2f} seconds\")\n    print(\"\\nClassification Report:\")\n    print(metrics['classification_report'])\n   \n    # Get ROC AUC scores\n    print(\"\\nROC AUC Scores:\")\n    for i in range(CONFIG['num_classes']):\n        print(f\"Class {i}: {metrics['roc_auc'][i]:.4f}\")\n    print(f\"Micro-average: {metrics['roc_auc']['micro']:.4f}\")\n    print(f\"Macro-average: {metrics['roc_auc']['macro']:.4f}\")\n   \n    # Plot loss curve\n    plt.figure(figsize=(10, 5))\n    plt.plot(history['train_loss'], label='Train Loss')\n    plt.plot(history['val_loss'], label='Val Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title(f'Loss Curves for Fold {fold_index} - {CONFIG[\"model_variant\"]}')\n    plt.savefig(f'{CONFIG[\"model_variant\"]}_loss_curve_fold_{fold_index}.png')\n    plt.close()\n   \n    # Plot accuracy curve\n    plt.figure(figsize=(10, 5))\n    plt.plot(history['train_acc'], label='Train Acc')\n    plt.plot(history['val_acc'], label='Val Acc')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.title(f'Accuracy Curves for Fold {fold_index} - {CONFIG[\"model_variant\"]}')\n    plt.savefig(f'{CONFIG[\"model_variant\"]}_acc_curve_fold_{fold_index}.png')\n    plt.close()\n   \n    # Test with a random image\n    random_test_results = test_random_image(model, images, labels, transform=val_transform)\n   \n    # Save model and results\n    torch.save(model.state_dict(), f'{CONFIG[\"model_variant\"]}_model_fold_{fold_index}.pth')\n    np.save(f'{CONFIG[\"model_variant\"]}_results_fold_{fold_index}.npy', metrics)\n   \n    return metrics\n\n# Run for all folds\nif __name__ == \"__main__\":\n    results_all = {}\n   \n    # Run for all 5 folds\n    for fold in range(1, 6):\n        try:\n            metrics = run_experiment(fold)\n            results_all[fold] = metrics\n        except Exception as e:\n            print(f\"Error in fold {fold}: {e}\")\n   \n    # Calculate and print average accuracy across all folds\n    accuracies = [results_all[fold]['accuracy'] for fold in results_all if fold in results_all]\n    if accuracies:\n        avg_accuracy = np.mean(accuracies)\n        print(f\"\\nAverage accuracy across all folds: {avg_accuracy:.4f}\")\n       \n        # Calculate average AUC across all folds\n        avg_auc_micro = np.mean([results_all[fold]['roc_auc']['micro'] for fold in results_all if fold in results_all])\n        avg_auc_macro = np.mean([results_all[fold]['roc_auc']['macro'] for fold in results_all if fold in results_all])\n        print(f\"Average micro-average AUC: {avg_auc_micro:.4f}\")\n        print(f\"Average macro-average AUC: {avg_auc_macro:.4f}\")\n       \n        # Model and configuration summary\n        print(\"\\nModel Configuration Summary:\")\n        print(f\"  Model variant: {CONFIG['model_variant']}\")\n        print(f\"  Batch size: {CONFIG['batch_size']}\")\n        print(f\"  Epochs: {CONFIG['num_epochs']}\")\n        print(f\"  Learning rate: {CONFIG['base_lr']}\")\n        print(f\"  Weight decay: {CONFIG['weight_decay']}\")\n        print(f\"  Scheduler: {CONFIG['scheduler']}\")\n        print(f\"  MixUp alpha: {CONFIG['mixup_alpha']}\")\n        print(f\"  Label smoothing: {CONFIG['label_smoothing']}\")\n        print(f\"  Dropout rate: {CONFIG['dropout_rate']}\")\n       \n    # Visualize learning curves across all folds\n        if results_all:\n    # Plot average accuracy across folds\n            plt.figure(figsize=(12, 6))\n            for fold in results_all:\n                if 'val_acc' in results_all[fold]:\n                    plt.plot(results_all[fold]['val_acc'], label=f'Fold {fold}')\n                elif 'accuracy' in results_all[fold]:\n            # If val_acc key doesn't exist but accuracy does, plot a single point\n                    plt.scatter(0, results_all[fold]['accuracy'], label=f'Fold {fold} (final)')\n            plt.xlabel('Epoch')\n            plt.ylabel('Validation Accuracy')\n            plt.title(f'Validation Accuracy Across Folds - {CONFIG[\"model_variant\"]}')\n            plt.legend()\n            plt.grid(True, linestyle='--', alpha=0.7)\n            plt.savefig(f'{CONFIG[\"model_variant\"]}_all_folds_accuracy.png')\n            plt.close()\n       \n        # Generate final summary visualization of class-wise metrics\n        class_names = ['Meningioma', 'Glioma', 'Pituitary']\n        class_aucs = []\n        for i in range(CONFIG['num_classes']):\n            class_auc = np.mean([results_all[fold]['roc_auc'][i] for fold in results_all if fold in results_all])\n            class_aucs.append(class_auc)\n       \n        plt.figure(figsize=(10, 6))\n        plt.bar(class_names, class_aucs, color=['skyblue', 'lightgreen', 'salmon'])\n        plt.ylabel('Average AUC')\n        plt.title(f'Average AUC by Class - {CONFIG[\"model_variant\"]}')\n        plt.ylim(0.8, 1.0)  # Adjust as needed\n        for i, v in enumerate(class_aucs):\n            plt.text(i, v + 0.01, f\"{v:.4f}\", ha='center')\n        plt.savefig(f'{CONFIG[\"model_variant\"]}_class_aucs.png')\n        plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T02:03:23.062483Z","iopub.execute_input":"2025-05-07T02:03:23.062782Z","iopub.status.idle":"2025-05-07T02:47:31.914312Z","shell.execute_reply.started":"2025-05-07T02:03:23.062755Z","shell.execute_reply":"2025-05-07T02:47:31.913432Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nTraining configuration:\n  batch_size: 16\n  num_epochs: 15\n  base_lr: 0.0005\n  weight_decay: 2e-05\n  scheduler: onecycle\n  mixup_alpha: 0.2\n  label_smoothing: 0.1\n  dropout_rate: 0.4\n  model_variant: efficient_cbam\n  num_classes: 3\nDownloading dataset...\nPath to dataset files: /kaggle/input/figshare-brain-tumor-dataset\n\n==================== RUNNING FOLD 1 ====================\n\nFound 3064 .mat files\nProcessed 0 images\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/426708811.py:93: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  lbl.append(int(images['label'][0]))\n","output_type":"stream"},{"name":"stdout","text":"Processed 500 images\nProcessed 1000 images\nProcessed 1500 images\nProcessed 2000 images\nProcessed 2500 images\nProcessed 3000 images\nFound cvind.mat at: /kaggle/input/figshare-brain-tumor-dataset/dataset/cvind.mat\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_b4_rwightman-23ab8bcd.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b4_rwightman-23ab8bcd.pth\n100%|██████████| 74.5M/74.5M [00:00<00:00, 158MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/15\n----------\nTrain Loss: 0.8517 Acc: 0.6525\nVal Loss: 0.5621 Acc: 0.8376\nNew best model saved with accuracy: 0.8376\n\nEpoch 2/15\n----------\nTrain Loss: 0.7223 Acc: 0.7543\nVal Loss: 0.4923 Acc: 0.9096\nNew best model saved with accuracy: 0.9096\n\nEpoch 3/15\n----------\nTrain Loss: 0.6604 Acc: 0.8039\nVal Loss: 0.4495 Acc: 0.9207\nNew best model saved with accuracy: 0.9207\n\nEpoch 4/15\n----------\nTrain Loss: 0.6452 Acc: 0.8151\nVal Loss: 0.4510 Acc: 0.9225\nNew best model saved with accuracy: 0.9225\n\nEpoch 5/15\n----------\nTrain Loss: 0.6355 Acc: 0.8213\nVal Loss: 0.4247 Acc: 0.9188\n\nEpoch 6/15\n----------\nTrain Loss: 0.5984 Acc: 0.8476\nVal Loss: 0.4291 Acc: 0.9280\nNew best model saved with accuracy: 0.9280\n\nEpoch 7/15\n----------\nTrain Loss: 0.5937 Acc: 0.8524\nVal Loss: 0.4028 Acc: 0.9428\nNew best model saved with accuracy: 0.9428\n\nEpoch 8/15\n----------\nTrain Loss: 0.6098 Acc: 0.8381\nVal Loss: 0.3928 Acc: 0.9483\nNew best model saved with accuracy: 0.9483\n\nEpoch 9/15\n----------\nTrain Loss: 0.5597 Acc: 0.8667\nVal Loss: 0.4153 Acc: 0.9373\n\nEpoch 10/15\n----------\nTrain Loss: 0.5823 Acc: 0.8645\nVal Loss: 0.3800 Acc: 0.9631\nNew best model saved with accuracy: 0.9631\n\nEpoch 11/15\n----------\nTrain Loss: 0.5779 Acc: 0.8620\nVal Loss: 0.3701 Acc: 0.9686\nNew best model saved with accuracy: 0.9686\n\nEpoch 12/15\n----------\nTrain Loss: 0.5466 Acc: 0.8793\nVal Loss: 0.3797 Acc: 0.9613\n\nEpoch 13/15\n----------\nTrain Loss: 0.5639 Acc: 0.8691\nVal Loss: 0.3669 Acc: 0.9649\n\nEpoch 14/15\n----------\nTrain Loss: 0.5780 Acc: 0.8601\nVal Loss: 0.3623 Acc: 0.9705\nNew best model saved with accuracy: 0.9705\n\nEpoch 15/15\n----------\nTrain Loss: 0.5459 Acc: 0.8732\nVal Loss: 0.3559 Acc: 0.9760\nNew best model saved with accuracy: 0.9760\n\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/426708811.py:564: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f'best_{CONFIG[\"model_variant\"]}_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Fold 1 accuracy: 0.9760\nTraining time: 585.90 seconds\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.96      0.95       113\n           1       0.99      0.97      0.98       288\n           2       0.98      0.99      0.99       141\n\n    accuracy                           0.98       542\n   macro avg       0.97      0.98      0.97       542\nweighted avg       0.98      0.98      0.98       542\n\n\nROC AUC Scores:\nClass 0: 0.9946\nClass 1: 0.9988\nClass 2: 0.9992\nMicro-average: 0.9983\nMacro-average: 0.9980\nRandom image test - True: Meningioma, Predicted: Meningioma\nConfidence scores: [0.689229  0.2733773 0.0373936]\n\n==================== RUNNING FOLD 2 ====================\n\nFound 3064 .mat files\nProcessed 0 images\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/426708811.py:93: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  lbl.append(int(images['label'][0]))\n","output_type":"stream"},{"name":"stdout","text":"Processed 500 images\nProcessed 1000 images\nProcessed 1500 images\nProcessed 2000 images\nProcessed 2500 images\nProcessed 3000 images\nFound cvind.mat at: /kaggle/input/figshare-brain-tumor-dataset/dataset/cvind.mat\nEpoch 1/15\n----------\nError in fold 2: Expected more than 1 value per channel when training, got input size torch.Size([1, 512])\n\n==================== RUNNING FOLD 3 ====================\n\nFound 3064 .mat files\nProcessed 0 images\nProcessed 500 images\nProcessed 1000 images\nProcessed 1500 images\nProcessed 2000 images\nProcessed 2500 images\nProcessed 3000 images\nFound cvind.mat at: /kaggle/input/figshare-brain-tumor-dataset/dataset/cvind.mat\nEpoch 1/15\n----------\nTrain Loss: 0.8465 Acc: 0.6575\nVal Loss: 0.5583 Acc: 0.8636\nNew best model saved with accuracy: 0.8636\n\nEpoch 2/15\n----------\nTrain Loss: 0.7114 Acc: 0.7641\nVal Loss: 0.4741 Acc: 0.9178\nNew best model saved with accuracy: 0.9178\n\nEpoch 3/15\n----------\nTrain Loss: 0.6666 Acc: 0.7970\nVal Loss: 0.4341 Acc: 0.9563\nNew best model saved with accuracy: 0.9563\n\nEpoch 4/15\n----------\nTrain Loss: 0.6426 Acc: 0.8150\nVal Loss: 0.4115 Acc: 0.9476\n\nEpoch 5/15\n----------\nTrain Loss: 0.6219 Acc: 0.8227\nVal Loss: 0.3962 Acc: 0.9598\nNew best model saved with accuracy: 0.9598\n\nEpoch 6/15\n----------\nTrain Loss: 0.6071 Acc: 0.8383\nVal Loss: 0.3935 Acc: 0.9476\n\nEpoch 7/15\n----------\nTrain Loss: 0.5967 Acc: 0.8429\nVal Loss: 0.3941 Acc: 0.9580\n\nEpoch 8/15\n----------\nTrain Loss: 0.5642 Acc: 0.8620\nVal Loss: 0.3934 Acc: 0.9563\n\nEpoch 9/15\n----------\nTrain Loss: 0.5560 Acc: 0.8658\nVal Loss: 0.3733 Acc: 0.9668\nNew best model saved with accuracy: 0.9668\n\nEpoch 10/15\n----------\nTrain Loss: 0.5522 Acc: 0.8733\nVal Loss: 0.3704 Acc: 0.9615\n\nEpoch 11/15\n----------\nTrain Loss: 0.5708 Acc: 0.8672\nVal Loss: 0.3827 Acc: 0.9598\n\nEpoch 12/15\n----------\nTrain Loss: 0.5644 Acc: 0.8725\nVal Loss: 0.3714 Acc: 0.9668\n\nEpoch 13/15\n----------\nTrain Loss: 0.5494 Acc: 0.8815\nVal Loss: 0.3748 Acc: 0.9703\nNew best model saved with accuracy: 0.9703\n\nEpoch 14/15\n----------\nTrain Loss: 0.5656 Acc: 0.8673\nVal Loss: 0.3690 Acc: 0.9650\n\nEpoch 15/15\n----------\nTrain Loss: 0.5478 Acc: 0.8780\nVal Loss: 0.3736 Acc: 0.9563\n\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/426708811.py:564: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f'best_{CONFIG[\"model_variant\"]}_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Fold 3 accuracy: 0.9703\nTraining time: 584.98 seconds\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.90      0.95      0.93        99\n           1       0.97      0.99      0.98       267\n           2       1.00      0.96      0.98       206\n\n    accuracy                           0.97       572\n   macro avg       0.96      0.97      0.96       572\nweighted avg       0.97      0.97      0.97       572\n\n\nROC AUC Scores:\nClass 0: 0.9913\nClass 1: 0.9968\nClass 2: 0.9992\nMicro-average: 0.9961\nMacro-average: 0.9961\nRandom image test - True: Pituitary, Predicted: Pituitary\nConfidence scores: [0.09842034 0.05244412 0.8491356 ]\n\n==================== RUNNING FOLD 4 ====================\n\nFound 3064 .mat files\nProcessed 0 images\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/426708811.py:93: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  lbl.append(int(images['label'][0]))\n","output_type":"stream"},{"name":"stdout","text":"Processed 500 images\nProcessed 1000 images\nProcessed 1500 images\nProcessed 2000 images\nProcessed 2500 images\nProcessed 3000 images\nFound cvind.mat at: /kaggle/input/figshare-brain-tumor-dataset/dataset/cvind.mat\nEpoch 1/15\n----------\nTrain Loss: 0.8498 Acc: 0.6592\nVal Loss: 0.6058 Acc: 0.8471\nNew best model saved with accuracy: 0.8471\n\nEpoch 2/15\n----------\nTrain Loss: 0.7172 Acc: 0.7647\nVal Loss: 0.5215 Acc: 0.8917\nNew best model saved with accuracy: 0.8917\n\nEpoch 3/15\n----------\nTrain Loss: 0.6582 Acc: 0.8007\nVal Loss: 0.4554 Acc: 0.9172\nNew best model saved with accuracy: 0.9172\n\nEpoch 4/15\n----------\nTrain Loss: 0.6155 Acc: 0.8369\nVal Loss: 0.4402 Acc: 0.9347\nNew best model saved with accuracy: 0.9347\n\nEpoch 5/15\n----------\nTrain Loss: 0.6136 Acc: 0.8385\nVal Loss: 0.4301 Acc: 0.9315\n\nEpoch 6/15\n----------\nTrain Loss: 0.5994 Acc: 0.8479\nVal Loss: 0.4160 Acc: 0.9506\nNew best model saved with accuracy: 0.9506\n\nEpoch 7/15\n----------\nTrain Loss: 0.5861 Acc: 0.8580\nVal Loss: 0.4100 Acc: 0.9347\n\nEpoch 8/15\n----------\nTrain Loss: 0.6003 Acc: 0.8451\nVal Loss: 0.4311 Acc: 0.9299\n\nEpoch 9/15\n----------\nTrain Loss: 0.5729 Acc: 0.8672\nVal Loss: 0.3822 Acc: 0.9554\nNew best model saved with accuracy: 0.9554\n\nEpoch 10/15\n----------\nTrain Loss: 0.5379 Acc: 0.8836\nVal Loss: 0.3771 Acc: 0.9634\nNew best model saved with accuracy: 0.9634\n\nEpoch 11/15\n----------\nTrain Loss: 0.5689 Acc: 0.8696\nVal Loss: 0.3993 Acc: 0.9554\n\nEpoch 12/15\n----------\nTrain Loss: 0.5617 Acc: 0.8651\nVal Loss: 0.3765 Acc: 0.9666\nNew best model saved with accuracy: 0.9666\n\nEpoch 13/15\n----------\nTrain Loss: 0.5609 Acc: 0.8730\nVal Loss: 0.3733 Acc: 0.9618\n\nEpoch 14/15\n----------\nTrain Loss: 0.5645 Acc: 0.8654\nVal Loss: 0.3805 Acc: 0.9634\n\nEpoch 15/15\n----------\nTrain Loss: 0.5455 Acc: 0.8819\nVal Loss: 0.3532 Acc: 0.9697\nNew best model saved with accuracy: 0.9697\n\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/426708811.py:564: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f'best_{CONFIG[\"model_variant\"]}_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Fold 4 accuracy: 0.9697\nTraining time: 578.60 seconds\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.97      0.95      0.96       168\n           1       0.98      0.98      0.98       287\n           2       0.96      0.98      0.97       173\n\n    accuracy                           0.97       628\n   macro avg       0.97      0.97      0.97       628\nweighted avg       0.97      0.97      0.97       628\n\n\nROC AUC Scores:\nClass 0: 0.9980\nClass 1: 0.9990\nClass 2: 0.9991\nMicro-average: 0.9987\nMacro-average: 0.9988\nRandom image test - True: Pituitary, Predicted: Pituitary\nConfidence scores: [0.06013858 0.03194652 0.9079149 ]\n\n==================== RUNNING FOLD 5 ====================\n\nFound 3064 .mat files\nProcessed 0 images\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/426708811.py:93: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  lbl.append(int(images['label'][0]))\n","output_type":"stream"},{"name":"stdout","text":"Processed 500 images\nProcessed 1000 images\nProcessed 1500 images\nProcessed 2000 images\nProcessed 2500 images\nProcessed 3000 images\nFound cvind.mat at: /kaggle/input/figshare-brain-tumor-dataset/dataset/cvind.mat\nEpoch 1/15\n----------\nTrain Loss: 0.8297 Acc: 0.6696\nVal Loss: 0.5952 Acc: 0.8351\nNew best model saved with accuracy: 0.8351\n\nEpoch 2/15\n----------\nTrain Loss: 0.7125 Acc: 0.7553\nVal Loss: 0.5240 Acc: 0.8865\nNew best model saved with accuracy: 0.8865\n\nEpoch 3/15\n----------\nTrain Loss: 0.6550 Acc: 0.7986\nVal Loss: 0.4622 Acc: 0.8927\nNew best model saved with accuracy: 0.8927\n\nEpoch 4/15\n----------\nTrain Loss: 0.6405 Acc: 0.8208\nVal Loss: 0.4355 Acc: 0.9238\nNew best model saved with accuracy: 0.9238\n\nEpoch 5/15\n----------\nTrain Loss: 0.6215 Acc: 0.8284\nVal Loss: 0.4365 Acc: 0.9269\nNew best model saved with accuracy: 0.9269\n\nEpoch 6/15\n----------\nTrain Loss: 0.6457 Acc: 0.8231\nVal Loss: 0.4379 Acc: 0.9362\nNew best model saved with accuracy: 0.9362\n\nEpoch 7/15\n----------\nTrain Loss: 0.6076 Acc: 0.8404\nVal Loss: 0.4306 Acc: 0.9160\n\nEpoch 8/15\n----------\nTrain Loss: 0.5901 Acc: 0.8583\nVal Loss: 0.4191 Acc: 0.9425\nNew best model saved with accuracy: 0.9425\n\nEpoch 9/15\n----------\nTrain Loss: 0.5931 Acc: 0.8500\nVal Loss: 0.4017 Acc: 0.9440\nNew best model saved with accuracy: 0.9440\n\nEpoch 10/15\n----------\nTrain Loss: 0.5986 Acc: 0.8442\nVal Loss: 0.3977 Acc: 0.9533\nNew best model saved with accuracy: 0.9533\n\nEpoch 11/15\n----------\nTrain Loss: 0.5808 Acc: 0.8582\nVal Loss: 0.4024 Acc: 0.9425\n\nEpoch 12/15\n----------\nTrain Loss: 0.5682 Acc: 0.8726\nVal Loss: 0.3950 Acc: 0.9456\n\nEpoch 13/15\n----------\nTrain Loss: 0.5387 Acc: 0.8871\nVal Loss: 0.3937 Acc: 0.9518\n\nEpoch 14/15\n----------\nTrain Loss: 0.5548 Acc: 0.8715\nVal Loss: 0.3944 Acc: 0.9440\n\nEpoch 15/15\n----------\nTrain Loss: 0.5440 Acc: 0.8820\nVal Loss: 0.4065 Acc: 0.9425\n\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/426708811.py:564: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f'best_{CONFIG[\"model_variant\"]}_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Fold 5 accuracy: 0.9533\nTraining time: 575.03 seconds\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.90      0.90      0.90       144\n           1       0.99      0.96      0.97       296\n           2       0.95      0.99      0.97       203\n\n    accuracy                           0.95       643\n   macro avg       0.94      0.95      0.95       643\nweighted avg       0.95      0.95      0.95       643\n\n\nROC AUC Scores:\nClass 0: 0.9903\nClass 1: 0.9978\nClass 2: 0.9971\nMicro-average: 0.9955\nMacro-average: 0.9955\nRandom image test - True: Pituitary, Predicted: Pituitary\nConfidence scores: [0.05183881 0.03130339 0.9168578 ]\n\nAverage accuracy across all folds: 0.9673\nAverage micro-average AUC: 0.9972\nAverage macro-average AUC: 0.9971\n\nModel Configuration Summary:\n  Model variant: efficient_cbam\n  Batch size: 16\n  Epochs: 15\n  Learning rate: 0.0005\n  Weight decay: 2e-05\n  Scheduler: onecycle\n  MixUp alpha: 0.2\n  Label smoothing: 0.1\n  Dropout rate: 0.4\n","output_type":"stream"}],"execution_count":2}]}